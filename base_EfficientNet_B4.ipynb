{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/PROFOUNDIVE/AIDHellCamp2/blob/dev2/baseline_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9VMzkxmHvVt"
   },
   "source": [
    "# 6공학관 층 분류하기 - AID 지옥캠프2\n",
    "\n",
    "- 작성된 환경: Window 11, Pip, Python 3.9, CUDA 11.8, VScode\n",
    "- Google Colab 등의 클라우드 컴퓨팅 환경에서는 다른 동작을 할 가능성 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xA-LxFDEHvVv"
   },
   "source": [
    "## 실행 전 사전 준비\n",
    "\n",
    "### 모듈\n",
    "- shell에서\n",
    "``` shell\n",
    "# Colab 환경은 필요한 모든 모듈이 설치되어 있어 설치할 필요 x\n",
    "```\n",
    "``` shell\n",
    "# 본인 GPU(CUDA)에 맞는 버전을 설치해주세요\n",
    "# 참고: https://pytorch.org/get-started/locally/\n",
    "# ex1) Pip, CPU 환경\n",
    "# pip3 install torch torchvision torchaudio\n",
    "# ex2) Conda, CUDA 12.4 환경\n",
    "# conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia\n",
    "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "```\n",
    "```shell\n",
    "pip3 install -U scikit-learn\n",
    "pip3 install -U matplotlib\n",
    "pip3 install tqdm\n",
    "```\n",
    "\n",
    "### 데이터 파일\n",
    "- 데이터 파일을 root 경로(baseline_code.ipynb가 있는 폴더)에 압축해제 해주세요\n",
    "- data link: https://www.kaggle.com/datasets/hyunseok21/jiokdata\n",
    "- colab 환경이라면 다음 [챕터](#colab-환경에서-kaggle파일-다운로드하기---로컬환경이라면-건너뛰기) 확인해주세요\n",
    "``` shell\n",
    "📁root\n",
    " ├─📁test\n",
    " ├─📁train\n",
    " │  ├─📁2F_train\n",
    " │  ├─📁3F_train\n",
    " │  ├─📁4F_train\n",
    " │  └─📁5F_train\n",
    " └─📜baseline_code.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvafV0B_HvVw",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Colab 환경에서 kaggle파일 다운로드하기 - **로컬환경이라면 건너뛰기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QT2CAGawNsuT"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "eXvLsRQIHvVw",
    "outputId": "169062ee-c4e4-4a80-e8eb-51ef83540767"
   },
   "outputs": [],
   "source": [
    "# 실행 시 업로드 버튼 활성화\n",
    "# kaggle에서 발급받은 api key가 담긴 json 파일 업로드\n",
    "COLAB = True\n",
    "try:\n",
    "  from google.colab import files\n",
    "  files.upload()\n",
    "except:\n",
    "  COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUa0ji1IHvVx"
   },
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VkKh7-w4HvVx",
    "outputId": "a90a03a9-004d-4fac-b1a5-d612fdb09de0"
   },
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "  import kagglehub\n",
    "  import shutil\n",
    "\n",
    "  # Download latest version\n",
    "  path = kagglehub.dataset_download(\"hyunseok21/jiokdata\")\n",
    "\n",
    "  print(\"Path to dataset files:\", path)\n",
    "  shutil.move(path+'./train', '/content/')\n",
    "  shutil.move(path+'./test', '/content/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fPXNtVgMs-We",
    "outputId": "4f8cb15c-9723-4ee1-e729-a5cab2a4c12d"
   },
   "outputs": [],
   "source": [
    "!pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2Wypn5NHvVy"
   },
   "source": [
    "# Train Data로 학습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e30tzBdzHvVy"
   },
   "source": [
    "## 모델 학습 (with train data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kvtqz_JNHvVy"
   },
   "source": [
    "### 모듈 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WC_30jqDHvVy"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZI5PWI6wHvVz"
   },
   "source": [
    "### CUDA 코어 테스트 & 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3P3lks_oHvVz",
    "outputId": "ab745907-f018-41ec-a2cd-d6875f3e9b4e"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  print(torch.cuda.get_device_name())\n",
    "  print(torch.__version__)\n",
    "  print(torch.version.cuda)\n",
    "  x = torch.randn(1).cuda()\n",
    "  print(x)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxnrzbp3HvV0"
   },
   "source": [
    "### 하이퍼 파라미터 (Hyper Parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SmNNietCHvV0"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "  # For Train\n",
    "  'epoch': 1,\n",
    "  'batch_size': 32,\n",
    "\n",
    "  # CPU worker\n",
    "  'workers': 8, # 본인 cpu 쓰레드의 절반 정도\n",
    "\n",
    "  # imgShow\n",
    "  'num_show_img': 5, # 데이터가 잘 로드 되었는지 확인 하는 셀에서 보여줄 데이터 개수, 학습과 관련 없음\n",
    "\n",
    "  # For Optimizer\n",
    "  'learning_rate': 0.001,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GPLVbFcHvV0"
   },
   "source": [
    "### 저장할 학습 완료 모델 파일 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJVG6OMOHvV0"
   },
   "outputs": [],
   "source": [
    "pt_file_name = 'model.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4cniuFfHvV0"
   },
   "source": [
    "### 데이터 라벨링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4BcCAMnHvV1"
   },
   "outputs": [],
   "source": [
    "class_names = {\n",
    "  \"0\": \"2F\",\n",
    "  \"1\": \"3F\",\n",
    "  \"2\": \"4F\",\n",
    "  \"3\": \"5F\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tu1_2UWJomXw"
   },
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dc4pTo3oolYe"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 0) 난수 시드 고정 (전체적으로 동일한 결과 재현 가능)\n",
    "# ------------------------------------------------------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "# torch.backends.cudnn.deterministic = True  # 필요시 활성화(학습 속도↓, 완전 재현성↑)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 1) 학습(train)용 증강 Transform & 검증/테스트용 Transform 정의\n",
    "# ------------------------------------------------------------------------\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((380, 380)),\n",
    "    transforms.RandomApply([\n",
    "        transforms.RandomResizedCrop((380, 380), scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(p=1.0),\n",
    "        transforms.RandomRotation(degrees=30),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2,\n",
    "                               saturation=0.2, hue=0.1)\n",
    "    ], p=0.8),  # 원하는 확률로 변환 적용\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# valid/test에서는 증강을 적용하지 않고, 리사이즈 & 정규화만 수행\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((380, 380)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 2) 전체 ImageFolder 불러오기 (transform은 일단 None)\n",
    "#    - 전체 이미지 인덱스(list(range(len(full_dataset))))를 먼저 구분\n",
    "# ------------------------------------------------------------------------\n",
    "full_dataset = datasets.ImageFolder(\n",
    "    root='./train',  # ImageFolder 구조의 이미지가 있는 폴더\n",
    "    transform=None   # 나중에 Subset으로 각각 다른 transform을 줄 예정\n",
    ")\n",
    "\n",
    "all_indices = list(range(len(full_dataset)))\n",
    "print(\"전체 이미지 개수:\", len(full_dataset))\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 3) 8:1:1 (train : valid : test) 분할\n",
    "#    - 첫 분할: train 80%, 나머지(tmp) 20%\n",
    "#    - 두 번째 분할: tmp 20% -> valid 10%, test 10%\n",
    "# ------------------------------------------------------------------------\n",
    "train_indices, tmp_indices = train_test_split(\n",
    "    all_indices, test_size=0.2, random_state=SEED, shuffle=True\n",
    ")\n",
    "val_indices, test_indices = train_test_split(\n",
    "    tmp_indices, test_size=0.5, random_state=SEED, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"train 개수: {len(train_indices)} | valid 개수: {len(val_indices)} | test 개수: {len(test_indices)}\")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 4) Subset을 통해 train, valid, test에 서로 다른 transform 적용\n",
    "#    - train -> train_transform (증강 포함)\n",
    "#    - valid, test -> eval_transform (증강 없음)\n",
    "#\n",
    "#    방법:\n",
    "#      datasets.ImageFolder(...)를 다시 각각 만들되,\n",
    "#      동일한 root 폴더를 사용하고 transform만 다르게 지정.\n",
    "#      그리고 Subset에 (원본 ImageFolder, 인덱스) 적용.\n",
    "# ------------------------------------------------------------------------\n",
    "train_dataset = Subset(\n",
    "    datasets.ImageFolder(root='./train', transform=train_transform),\n",
    "    train_indices\n",
    ")\n",
    "\n",
    "val_dataset = Subset(\n",
    "    datasets.ImageFolder(root='./train', transform=eval_transform),\n",
    "    val_indices\n",
    ")\n",
    "\n",
    "test_dataset = Subset(\n",
    "    datasets.ImageFolder(root='./train', transform=eval_transform),\n",
    "    test_indices\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 5) DataLoader 정의\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_dataset,\n",
    "                        batch_size=params['batch_size'],\n",
    "                        shuffle=True,\n",
    "                        num_workers=params['workers'],\n",
    "                        pin_memory=True),\n",
    "    'valid': DataLoader(val_dataset,\n",
    "                        batch_size=params['batch_size'],\n",
    "                        shuffle=False,\n",
    "                        num_workers=params['workers'],\n",
    "                        pin_memory=True),\n",
    "    'test': DataLoader(test_dataset,\n",
    "                       batch_size=params['batch_size'],\n",
    "                       shuffle=False,\n",
    "                       num_workers=params['workers'],\n",
    "                       pin_memory=True)\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 6) 배치 수 계산 및 출력\n",
    "# ------------------------------------------------------------------------\n",
    "batch_num = {\n",
    "    'train': len(dataloaders['train']),\n",
    "    'valid': len(dataloaders['valid']),\n",
    "    'test': len(dataloaders['test'])\n",
    "}\n",
    "\n",
    "print('batch_size: %d | batch_num(train/valid/test): %d / %d / %d' %\n",
    "      (params['batch_size'],\n",
    "       batch_num['train'],\n",
    "       batch_num['valid'],\n",
    "       batch_num['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhP0DZD0HvV1"
   },
   "source": [
    "### Train 함수\n",
    "Args:\n",
    "    model: 학습할 모델.\n",
    "    criterion: 손실 함수.\n",
    "    optimizer: 최적화 알고리즘.\n",
    "    dataloaders: 'train'과 'valid' 단계의 DataLoader.\n",
    "    num_epochs (int): 학습할 에포크 수.\n",
    "    checkpoint_path (str): 체크포인트를 저장할 경로.\n",
    "\n",
    "Returns:\n",
    "    model: 최적의 가중치를 가진 모델.\n",
    "    best_idx (int): 최고의 검증 정확도를 달성한 에포크 번호.\n",
    "    best_acc (float): 최고의 검증 정확도.\n",
    "    train_loss (list): 각 에포크의 학습 손실.\n",
    "    train_acc (list): 각 에포크의 학습 정확도.\n",
    "    valid_loss (list): 각 에포크의 검증 손실.\n",
    "    valid_acc (list): 각 에포크의 검증 정확도."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_SMZMsvHvV1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to load the model and optimizer from a checkpoint file\n",
    "def load_checkpoint(model, optimizer, checkpoint_path, device):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "    # Load model state dict\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # Load optimizer state dict\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    # Load best accuracy and epoch\n",
    "    best_acc = checkpoint['best_acc']\n",
    "    epoch = checkpoint['epoch']\n",
    "\n",
    "    return model, optimizer, best_acc, epoch\n",
    "\n",
    "def train_model(model, criterion, optimizer, dataloaders: dict, num_epochs=25, checkpoint_path=None):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_idx = None  # <-- Needs to be initialized\n",
    "    train_loss, train_acc, valid_loss, valid_acc = [], [], [], []\n",
    "\n",
    "    # Load checkpoint if available\n",
    "    start_epoch = 0\n",
    "    if checkpoint_path:\n",
    "        model, optimizer, best_acc, start_epoch = load_checkpoint(model, optimizer, checkpoint_path, device)\n",
    "        print(f\"Resuming training from epoch {start_epoch + 1} with best validation accuracy {best_acc:.1f}%\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):  # Start from the last saved epoch\n",
    "        print()\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}:', '-' * 10)\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluation mode\n",
    "\n",
    "            running_loss, running_corrects, num_cnt = 0.0, 0, 0\n",
    "\n",
    "            count = 1\n",
    "            pbar_dataloaders = tqdm(dataloaders[phase], desc=phase, ncols=70)\n",
    "            for inputs, labels in pbar_dataloaders:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                num_cnt += len(labels)\n",
    "                count += 1\n",
    "            pbar_dataloaders.close()\n",
    "\n",
    "            epoch_loss = running_loss / num_cnt\n",
    "            epoch_acc = (running_corrects.double() / num_cnt).cpu() * 100\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_loss.append(epoch_loss)\n",
    "                train_acc.append(epoch_acc)\n",
    "            else:\n",
    "                valid_loss.append(epoch_loss)\n",
    "                valid_acc.append(epoch_acc)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.2f} Acc: {epoch_acc:.1f}')\n",
    "\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_idx = epoch + 1\n",
    "                best_acc = epoch_acc\n",
    "                print(f'==> best model saved - Epoch {epoch + 1} / {best_acc:.1f}%')\n",
    "                torch.save({\n",
    "                    'epoch': num_epochs + 1,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'best_acc': best_acc,\n",
    "                }, pt_file_name)\n",
    "                print('Checkpoint saved')\n",
    "\n",
    "    # Final result\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best valid Acc: Epoch {start_epoch + 1} / {best_acc:.1f}%')\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, best_idx, best_acc, train_loss, train_acc, valid_loss, valid_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIcwKIsiHvV2"
   },
   "source": [
    "### 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tNVJUk3qHvV3"
   },
   "outputs": [],
   "source": [
    "def imgShow(input, title=None):\n",
    "  \"\"\"Img show for Tensor.\"\"\"\n",
    "  input = input.numpy().transpose((1,2,0))\n",
    "  input = np.clip(input, 0, 1)\n",
    "  plt.imshow(input)\n",
    "  if title is not None:\n",
    "    plt.title(title)\n",
    "  plt.pause(0.001) # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "5G6VBhOGHvV3",
    "outputId": "da26d3b4-c1d7-4219-e845-ce90b80d3dbf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "out = torchvision.utils.make_grid(inputs[:params['num_show_img']])\n",
    "imgShow(out, title=[class_names[str(int(x))] for x in classes[:params['num_show_img']]])\n",
    "# valid data\n",
    "inputs, classes = next(iter(dataloaders['valid']))\n",
    "out = torchvision.utils.make_grid(inputs[:params['num_show_img']])\n",
    "imgShow(out, title=[class_names[str(int(x))] for x in classes[:params['num_show_img']]])\n",
    "# test data\n",
    "inputs, classes = next(iter(dataloaders['test']))\n",
    "out = torchvision.utils.make_grid(inputs[:params['num_show_img']])\n",
    "imgShow(out, title=[class_names[str(int(x))] for x in classes[:params['num_show_img']]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0V-snMeHvV3"
   },
   "source": [
    "### 모델을 장치로 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r8KwQ5k7HvV3",
    "outputId": "9532d39d-6f40-4ed9-8e4c-ba540e324ea1"
   },
   "outputs": [],
   "source": [
    "# EfficientNet model setup\n",
    "from torchvision.models import efficientnet_b4\n",
    "\n",
    "model = efficientnet_b4(pretrained=True)\n",
    "num_classes = 4\n",
    "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDT5t6tLHvV3"
   },
   "source": [
    "### 손실함수 & 옵티마이저\n",
    "\n",
    "Adam:\n",
    "대부분의 경우 기본값으로 사용해도 잘 작동.  \n",
    "SGD with Momentum:\n",
    "데이터셋이 크거나, 안정적인 학습이 중요할 때 유리.  \n",
    "AdamW:\n",
    "EfficientNet 등 최신 아키텍처와 조합 시 추천.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xMlUIMBpHvV4"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ll4E6jvFHvV4"
   },
   "source": [
    "### 학습 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1idPP2UnHvV4",
    "outputId": "547e284b-fe8b-4322-ab50-cab9b46d8540",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load checkpoint if exists, otherwise train from scratch\n",
    "checkpoint_path = pt_file_name if os.path.exists(pt_file_name) else None\n",
    "\n",
    "model, best_idx, best_acc, train_loss, train_acc, valid_loss, valid_acc = train_model(\n",
    "    model, criterion, optimizer, dataloaders, num_epochs=params['epoch'], checkpoint_path=checkpoint_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5SOTyCVHvV4"
   },
   "source": [
    "## 학습 결과 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "8mqavVrGHvV4",
    "outputId": "38711e5b-ae52-477a-f60e-6c92ef71afed"
   },
   "outputs": [],
   "source": [
    "## 결과 그래프 그리기\n",
    "print('best model info:\\nModel extracted from epoch %d\\nValid Acc=%1.f / Valid Loss=%.1f'%(best_idx, valid_acc[best_idx-1], valid_loss[best_idx-1]))\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot([n for n in range(1,len(train_acc)+1)], train_acc, 'b-', label='train acc')\n",
    "ax1.plot([n for n in range(1,len(valid_acc)+1)], valid_acc, 'r-', label ='valid_acc')\n",
    "plt.plot(best_idx, valid_acc[best_idx-1], 'ro')\n",
    "ax1.set_xlabel('epoch')\n",
    "# Make the y-axis label, ticks and tick labels match the line color.\n",
    "ax1.set_ylabel('acc', color='k')\n",
    "ax1.tick_params('y', colors='k')\n",
    "plt.legend(bbox_to_anchor=(-0.1, 1.0), loc=\"upper right\")\n",
    "plt.grid()\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot([n for n in range(1,len(train_loss)+1)], train_loss, 'g-', label='train loss')\n",
    "ax2.plot([n for n in range(1,len(valid_loss)+1)], valid_loss, 'k-', label='valid loss')\n",
    "plt.plot(best_idx, valid_loss[best_idx-1], 'ro')\n",
    "ax2.set_ylabel('loss', color='k')\n",
    "ax2.tick_params('y', colors='k')\n",
    "plt.legend(bbox_to_anchor=(1.1, 1.0), loc=\"upper left\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HXLtv53HvV9"
   },
   "source": [
    "## 모델 TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E90EexDhHvV9"
   },
   "outputs": [],
   "source": [
    "def test_and_visualize_model(model, dataloaders, phase = 'test', num_images=4):\n",
    "  # phase = 'train', 'valid', 'test'\n",
    "\n",
    "  was_training = model.training\n",
    "  model.eval()\n",
    "\n",
    "  running_loss, running_corrects, num_cnt = 0.0, 0, 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "      inputs = inputs.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      outputs = model(inputs)\n",
    "      _, preds = torch.max(outputs, 1)\n",
    "      loss = criterion(outputs, labels) # batch의 평균 loss 출력\n",
    "\n",
    "      running_loss += loss.item() * inputs.size(0)\n",
    "      running_corrects += torch.sum(preds == labels.data)\n",
    "      num_cnt += inputs.size(0) # batch size\n",
    "\n",
    "    test_loss = running_loss / num_cnt\n",
    "    test_acc = running_corrects.double() / num_cnt\n",
    "    print('test done : loss/acc : %.2f / %.1f' % (test_loss, test_acc*100))\n",
    "\n",
    "  # 예시 그림 출력\n",
    "  with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "      inputs = inputs.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      outputs = model(inputs)\n",
    "      _, preds = torch.max(outputs, 1)\n",
    "\n",
    "      for j in range(1, num_images+1):\n",
    "        ax = plt.subplot(num_images//2, 2, j)\n",
    "        ax.axis('off')\n",
    "        ax.set_title('%s : %s -> %s' %(\n",
    "          'True' if class_names[str(labels[j].cpu().numpy())] == class_names[str(preds[j].cpu().numpy())] else 'False',\n",
    "          class_names[str(labels[j].cpu().numpy())],\n",
    "          class_names[str(preds[j].cpu().numpy())]\n",
    "        ))\n",
    "        imgShow(inputs.cpu().data[j])\n",
    "\n",
    "      if i == 0: break\n",
    "\n",
    "  model.train(mode=was_training) # 다시 train모드로 전환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 875
    },
    "id": "c21VSE4LHvV9",
    "outputId": "1f99ccc0-fbf7-4a07-8ae7-ba4e9ed04565"
   },
   "outputs": [],
   "source": [
    "test_and_visualize_model(model, dataloaders, phase='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meb4NRw2HvV9"
   },
   "source": [
    "# Test 데이터 분류 및 CSV 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XbI2FyEHvV-"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-QrfLvaHvV-"
   },
   "outputs": [],
   "source": [
    "data_path = './test'\n",
    "image_files = sorted(glob.glob(data_path + '/*'))\n",
    "csv_filename = 'answer.csv'\n",
    "\n",
    "os_name = sys.platform\n",
    "path_split = '/'\n",
    "if os_name.startswith('win'):\n",
    "  path_split = '\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qjb1SLhgHvV-"
   },
   "outputs": [],
   "source": [
    "class TestImageDataset(Dataset):\n",
    "  def __init__(self, files, transform):\n",
    "    super().__init__()\n",
    "    self.files = files\n",
    "    self.transform = transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.files)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    file = self.files[index]\n",
    "    try:\n",
    "        img = Image.open(file).convert('RGB')\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening image {file}: {e}\")\n",
    "        raise\n",
    "    img = self.transform(img)\n",
    "\n",
    "    return img, file.split(path_split)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QkxBb-IrHvV-"
   },
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "                        transforms.Resize((380, 380)), # 224x224 사이즈로 변환\n",
    "                        transforms.ToTensor(), # 텐서화\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gw3669KGHvV-"
   },
   "outputs": [],
   "source": [
    "test_dataset = TestImageDataset(image_files, test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U--nWaMlHvV_"
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=params['batch_size'],\n",
    "                         shuffle=False,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zg2LU0XbHvV_",
    "outputId": "19dc3d54-d0df-4446-a42c-b1661c470d9c"
   },
   "outputs": [],
   "source": [
    "model_dict = torch.load(pt_file_name, map_location=device)\n",
    "\n",
    "trained_model = efficientnet_b4(pretrained=True)  # pretrained=True로 사전 학습된 가중치 로드\n",
    "trained_model.classifier[1] = torch.nn.Linear(trained_model.classifier[1].in_features, num_classes)\n",
    "trained_model.load_state_dict(model_dict['model_state_dict'])\n",
    "trained_model = trained_model.to(device)\n",
    "\n",
    "# CUDA 확인\n",
    "print(f\"CUDA:{next(trained_model.parameters()).is_cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IqRqrUsPHvV_"
   },
   "outputs": [],
   "source": [
    "def test_and_make_csv(model, dataloader):\n",
    "  # phase = 'train', 'valid', 'test'\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  answer_csv = 'id,floor\\n'\n",
    "\n",
    "  pbar_dataloaders = tqdm(dataloader,\n",
    "                          desc='Making Csv',\n",
    "                          ncols=70)\n",
    "  with torch.no_grad():\n",
    "    for inputs, filenames in pbar_dataloaders:\n",
    "      inputs = inputs.to(device)\n",
    "\n",
    "      outputs = model(inputs)\n",
    "      _, preds = torch.max(outputs, 1)\n",
    "\n",
    "      for j in range(len(preds)):\n",
    "        answer_csv += f'{filenames[j].split(\".\")[0]},{class_names[str(preds[j].cpu().numpy())]}\\n'\n",
    "  pbar_dataloaders.close()\n",
    "\n",
    "  with open(csv_filename, 'w') as file:\n",
    "    file.write(answer_csv)\n",
    "    print(f'{csv_filename}가 생성되었습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mpt3uDTwHvV_",
    "outputId": "0eae30d2-c4fa-4c75-d419-0b3eb3cb8019"
   },
   "outputs": [],
   "source": [
    "test_and_make_csv(trained_model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6504703,
     "sourceId": 10507024,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python [conda env:torchdev]",
   "language": "python",
   "name": "conda-env-torchdev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
